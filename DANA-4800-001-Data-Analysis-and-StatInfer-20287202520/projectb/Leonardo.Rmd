---
title: "P"
author: "L"
date: "2025-07-18"
output: html_document
---

# ðŸ“„ Column Descriptions

-   **`id`**\
    Row ID consisting of the participant number and a count for that participant.

-   **`p_num`**\
    Participant number.

-   **`time`**\
    Time of day in the format `HH:MM:SS`.

-   **`bg-X:XX`**\
    Blood glucose reading in mmol/L, where `X:XX (H:MM)` is the time in the past\
    (e.g., `bg-2:35` means the reading from **2 hours and 35 minutes before** the time value for that row),\
    recorded by the continuous glucose monitor.

-   **`insulin-X:XX`**\
    Total insulin dose received in **units** in the last 5 minutes. `X:XX` refers to the time in the past\
    (e.g., `insulin-2:35` is the insulin dose **between 2:40 and 2:35 before** the time value),\
    recorded by the insulin pump.

-   **`carbs-X:XX`**\
    Total carbohydrate intake in **grams** in the last 5 minutes. `X:XX` indicates the time window in the past\
    (e.g., `carbs-2:35` is the amount **between 2:40 and 2:35 before** the time value),\
    recorded by the participant.

-   **`hr-X:XX`**\
    Mean heart rate in **beats per minute** in the last 5 minutes. `X:XX` represents the past time window\
    (e.g., `hr-2:35` is the mean heart rate **between 2:40 and 2:35 before** the time value),\
    recorded by the smartwatch.

-   **`steps-X:XX`**\
    Total steps walked in the last 5 minutes. `X:XX` indicates the time in the past\
    (e.g., `steps-2:35` = steps **between 2:40 and 2:35 before** the row's time),\
    recorded by the smartwatch.

-   **`cals-X:XX`**\
    Total calories burned in the last 5 minutes. `X:XX` refers to time in the past\
    (e.g., `cals-2:35` = calories **burned between 2:40 and 2:35 before** the row time),\
    calculated by the smartwatch.

-   **`activity-X:XX`**\
    Self-declared activity performed in the last 5 minutes. `X:XX` refers to time in the past\
    (e.g., `activity-2:35` shows the **activity name between 2:40 and 2:35 before** the row time),\
    set manually on the smartwatch.

-   **`bg+1:00`**\
    Blood glucose reading in mmol/L **one hour in the future** â€” this is the **target variable** to be predicted\
    (note: not provided in `test.csv`).

### Loading dataset

```{r}
library(dplyr)

sample <- read.csv("sample_submission.csv", header = TRUE) ### Sample
test <- read.csv("test.csv", header = TRUE) ### Test
train <- read.csv("train.csv", header = TRUE) ### Train

```

# understanding dataset

# check null

# check same values

# confirm number of people uniques by pnum

```{r}
dim(sample)
dim(test)
dim(train)

#str(train)
#glimpse(train)
#summary(train)
#install.packages("dplyr")
#install.packages("DT")

library(dplyr)
library(purrr)
library(DT)
# Assuming your data frame is named df
df<-train
total_rows <- nrow(df)

# Build column-wise summary
column_summary <- purrr::map_dfr(names(df), function(col) {
  column_data <- df[[col]]
  data_type <- class(column_data)[1]
  null_count <- sum(is.na(column_data))
  null_percent <- round((null_count / total_rows) * 100, 2)
  unique_count <- n_distinct(column_data)
  
  example_value <- as.character(column_data[which(!is.na(column_data))[1]])
  mode_value <- as.character(names(sort(table(column_data), decreasing = TRUE))[1])
  
  tibble(
    Column = col,
    Type = data_type,
    Mode = mode_value,
    Nulls = null_count,
    `% Null` = null_percent,
    Unique = unique_count,
    Example = example_value
  )
})


# Arrange by % Null descending (optional)
column_summary <- column_summary %>% arrange(desc(`% Null`))
# Count values in p_num
pnum_counts <- table(df$p_num)

print(unique(df$p_num))

# Print full summary
DT::datatable(
  column_summary,
  options = list(pageLength = 100),  # Show 100 rows per page
  rownames = TRUE
)



```

### merging dataset

```{r}

df_full <- bind_rows(train, test)
dim(df_full)

```

```{r}

# install.packages("dplyr")
library(dplyr)
df_full2 <- df_full %>% full_join(sample, by = "id")
head(df_full2)


```

```{r}

dim(df_full2)

```

```{r}

# Load necessary library
library(dplyr)


df <- df_full
total_rows <- nrow(df)

# Null summary: count, percentage, and non-null percentage
null_counts <- colSums(is.na(df))
non_null_counts <- total_rows - null_counts
null_percentages <- round(null_counts / total_rows * 100, 2)
non_null_percentages <- round(non_null_counts / total_rows * 100, 2)

# Combine into a data frame
null_summary <- data.frame(
  Column = names(null_counts),
  Null_Count = null_counts,
  Non_Null_Count = non_null_counts,
  Null_Percentage = null_percentages,
  Non_Null_Percentage = non_null_percentages,
  Possible_to_Delete = null_percentages > 50
)

# Sort by percentage of nulls descending
null_summary_sorted <- null_summary %>%
  arrange(desc(Null_Percentage))

# Show result
print(null_summary_sorted)


```
```{r}
library(dplyr)
library(tidyr)

df <- train  # or df_full

# Count total rows per user
rows_per_user <- df %>%
  group_by(p_num) %>%
  summarise(n_rows = n(), .groups = "drop")

# Count nulls and calculate percentages
nulls_per_user <- df %>%
  group_by(p_num) %>%
  summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}")) %>%
  ungroup()

# Join to get row counts
nulls_per_user <- nulls_per_user %>%
  left_join(rows_per_user, by = "p_num")

# Convert to long format for better inspection (null counts)
nulls_long <- nulls_per_user %>%
  pivot_longer(cols = starts_with("na_"), names_to = "column", values_to = "na_count") %>%
  mutate(column = gsub("^na_", "", column))

# Calculate % of nulls per user-column
nulls_long <- nulls_long %>%
  mutate(null_percent = round((na_count / n_rows) * 100, 2)) %>%
  arrange(desc(null_percent))

# Optional: Show wide format (just counts)
nulls_wide_counts <- nulls_long %>%
  select(p_num, column, na_count) %>%
  pivot_wider(names_from = column, values_from = na_count)

# Optional: Show wide format (percentages)
nulls_wide_percent <- nulls_long %>%
  select(p_num, column, null_percent) %>%
  pivot_wider(names_from = column, values_from = null_percent)

# Show result
print(nulls_wide_percent)

```
```{r}
column_null_summary <- nulls_long %>%
  group_by(column) %>%
  summarise(
    total_nulls = sum(na_count),
    avg_percent_nulls = mean(null_percent)
  ) %>%
  arrange(desc(total_nulls))

# View: columns with most missing values
print(column_null_summary)

user_null_summary <- nulls_long %>%
  group_by(p_num) %>%
  summarise(
    total_nulls = sum(na_count),
    avg_percent_nulls = mean(null_percent)
  ) %>%
  arrange(desc(total_nulls))

# View: users with worst data quality
# print(user_null_summary)
# âœ… (Optional) Visualizations
#  ðŸ“Š Columns with most nulls
library(ggplot2)

ggplot(column_null_summary, aes(x = reorder(column, -total_nulls), y = total_nulls)) +
  geom_col(fill = "firebrick") +
  labs(title = "Total Nulls per Column", x = "Column", y = "Missing Values") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
# ðŸ‘¤ Users with most nulls

ggplot(user_null_summary, aes(x = reorder(p_num, -total_nulls), y = total_nulls)) +
  geom_col(fill = "steelblue") +
  labs(title = "Total Nulls per User", x = "Participant", y = "Missing Values") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


library(ggplot2)

top_n <- 20

ggplot(head(column_null_summary, top_n), aes(x = reorder(column, -total_nulls), y = total_nulls)) +
  geom_col(fill = "firebrick") +
  labs(title = paste("Top", top_n, "Columns with Most Nulls"), x = "Column", y = "Missing Values") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
```{r}

# check duplicates
# View duplicated values in the id column
duplicated_ids <- df %>%filter(duplicated(id))

# Show how many duplicated IDs
num_duplicates <- nrow(duplicated_ids)
cat("ðŸ” Number of duplicated IDs:", num_duplicates, "\n")

# Show them
print(duplicated_ids)

# Unique duplicated id values
duplicate_id_values <- df$id[duplicated(df$id)] %>% unique()
print(duplicate_id_values)


all(names(train) == names(test))  
# Columns in train but not in test
diff_train <- setdiff(names(train), names(test))

# Columns in test but not in train
diff_test <- setdiff(names(test), names(train))

# Print differences
cat("ðŸŸ  Columns only in train:\n")
print(diff_train)

cat("\nðŸ”µ Columns only in test:\n")
print(diff_test)
```

```{r}
library(dplyr)

# Select numeric columns
numeric_df <- df %>% select(where(is.numeric))

# Loop over each numeric column
for (col_name in names(numeric_df)) {
  col_data <- numeric_df[[col_name]]
  
  # Set up 2 plots side by side in the RStudio viewer
  par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))
  
  # Boxplot
  boxplot(
    col_data,
    main = paste("Boxplot -", col_name),
    col = "orange",
    horizontal = TRUE
  )
  
  # Histogram
  hist(
    col_data,
    main = paste("Histogram -", col_name),
    xlab = col_name,
    col = "skyblue",
    breaks = 50
  )
  
  # readline(prompt = "Press [Enter] to continue...")
 Sys.sleep(0.5)  # Pause for 0.5 seconds
}


```

```{r}
# Load libraries
library(dplyr)

# Define function to detect outliers using IQR
detect_outliers <- function(data, column) {
  Q1 <- quantile(data[[column]], 0.25, na.rm = TRUE)
  Q3 <- quantile(data[[column]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Return rows where the value is an outlier
  data %>%
    filter((.data[[column]] < lower_bound | .data[[column]] > upper_bound) & !is.na(.data[[column]]))
}

# Create a named list to store outlier counts
outlier_counts <- list()

# Loop over numeric columns
numeric_columns <- df %>% select(where(is.numeric)) %>% names()

for (col in numeric_columns) {
  outliers <- detect_outliers(df, col)
  outlier_counts[[col]] <- nrow(outliers)
}

# Convert to data frame for easier viewing
outlier_summary <- data.frame(
  Column = names(outlier_counts),
  Outlier_Count = unlist(outlier_counts)
)

# Sort descending
outlier_summary <- outlier_summary %>%
  arrange(desc(Outlier_Count))

# Print full summary
DT::datatable(
  outlier_summary,
  options = list(pageLength = 100),  # Show 100 rows per page
  rownames = FALSE
)


```

```{r}
#install.packages("reshape2")
library(dplyr)
library(ggplot2)
library(reshape2)


df<-train
numeric_df <- df %>% select(where(is.numeric))

# Optional: Reduce number of columns (select only bg, insulin, or top 30 vars with most variation)
top_vars <- names(sort(sapply(numeric_df, function(x) sd(x, na.rm = TRUE)), decreasing = TRUE)[1:30])
cor_matrix <- cor(numeric_df[, top_vars], use = "complete.obs")

# Melt for ggplot
melted_cor <- melt(cor_matrix)

# Plot heatmap
ggplot(melted_cor, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red", midpoint = 0,
    name = "Correlation"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Correlation Heatmap of Top 30 Numeric Variables")

```

```{r}
library(dplyr)
library(lubridate)

# Convertimos la columna time en objeto tipo hms
df <- train %>%
  mutate(time_hms = hms(time))
```

```{r}
# Ordenamos por participante y hora
df <- df %>%
  arrange(p_num, row_number())  # orden confiable sin fecha

# Creamos la columna `day`
df <- df %>%
  group_by(p_num) %>%
  mutate(
    hour = hour(time_hms),
    day_num = cumsum(hour < lag(hour, default = first(hour))) + 1,
    day_label = paste0("Day ", day_num)
  ) %>%
  ungroup()  



```

```{r}
df_p01_time_day <- df %>%
  filter(p_num == "p01") %>%
  select(1:2, time, day_label, everything()) %>%  # agrega todas las columnas
# Mostrar todas las filas
print(df_p01_time_day, n = nrow(df_p01_time_day))
```
```{r}
df_summary_by_day <- df %>%
  group_by(p_num, day_label) %>%
  summarise(
    n_rows = n(),
    .groups = "drop"
  ) %>%
  mutate(day_num = as.integer(gsub("Day ", "", day_label))) %>%
  arrange(p_num, day_num) %>%
  select(-day_num)  # opcional: elimina columna auxiliar

print(df_summary_by_day)

```

```{r}
# LibrerÃ­as necesarias
library(dplyr)
library(lubridate)
library(ggplot2)

# --- PASO 1: Preparar los datos y calcular columna 'day' --

# --- PASO 2: Resumen por dÃ­a y usuario ---
df_summary_by_day <- df %>%
  group_by(p_num, day_label) %>%
  summarise(n_rows = n(), .groups = "drop") %>%
  mutate(day_num = as.integer(gsub("Day ", "", day_label))) %>%
  arrange(p_num, day_num)

# --- PASO 3: GrÃ¡fico de usuarios con mÃ¡s dÃ­as reportados ---
df_summary_by_day %>%
  count(p_num, name = "n_days") %>%
  ggplot(aes(x = reorder(p_num, -n_days), y = n_days)) +
  geom_col(fill = "steelblue") +
  labs(title = "Usuarios con mÃ¡s dÃ­as reportados",
       x = "Usuario",
       y = "NÃºmero de dÃ­as") +
  theme_minimal()


```
```{r}

# Asegurar que el dÃ­a sea ordenado numÃ©ricamente
df_summary_by_day <- df_summary_by_day %>%
  mutate(day_num = as.numeric(gsub("Day ", "", day_label))) %>%
  arrange(p_num, day_num)

# Obtener lista de usuarios
usuarios <- unique(df_summary_by_day$p_num)

# Crear un grÃ¡fico por usuario
for (usuario in usuarios) {
  df_user <- df_summary_by_day %>% filter(p_num == usuario)
  
  total_usuario <- sum(df_user$n_rows, na.rm = TRUE)
  
  plot <- ggplot(df_user, aes(x = day_num, y = n_rows)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "darkblue", size = 2) +
  geom_text(aes(label = n_rows), vjust = -1, size = 3.5) +
  scale_x_continuous(
    breaks = seq(min(df_user$day_num), max(df_user$day_num), by = 1),
    expand = expansion(mult = c(0.005, 0.02))  # MÃS espacio a los extremos X
  ) +
  scale_y_continuous(
    limits = c(min(df_user$n_rows) - 20, max(df_user$n_rows) + 10),
    breaks = seq(min(df_user$n_rows), max(df_user$n_rows), by = 20),
    expand = expansion(mult = c(0.05, 0.1))
  ) +
  labs(
    title = paste("EvoluciÃ³n diaria de registros por", usuario," ",total_usuario),
    x = "DÃ­a",
    y = "NÃºmero de filas"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12)
  )

  print(plot)
  
  # Opcional: guardar cada grÃ¡fico
  # ggsave(filename = paste0("reporte_", usuario, ".png"), plot = plot, width = 6, height = 4)
}

```
```{r}
# Seleccionar solo las columnas de glucosa (bg-)
df_p05 <- df[df$p_num == "p05", ]

bg_cols <- grep("^bg.", names(df_p05), value = TRUE)
df_bg <- df[, bg_cols]

# Crear funciÃ³n para comparar filas desplazadas
check_shifted_rows <- function(df) {
  shifted_indices <- c()
  for (i in 2:nrow(df)) {
    current <- as.character(df[i, 1:(ncol(df)-1)])
    previous <- as.character(df[i-1, 2:ncol(df)])
    if (all(current == previous, na.rm = TRUE)) {
      shifted_indices <- c(shifted_indices, i)
    }
  }
  return(shifted_indices)
}

# Ejecutar validaciÃ³n
shifted_rows <- check_shifted_rows(df_bg)

# Mostrar resultados
cat("Filas desplazadas detectadas:", length(shifted_rows), "\n")
print(head(shifted_rows, 10))
print(shifted_rows)

```

