**Student A:**
\[
P(A=\text{Pass}) = 2\beta
\]
**Student B:**
\[
P(B=\text{Fail}) = 1 - 2\beta
\]
**Student C:**
\[
P(C=\text{Pass}) = 3\beta
\]
**Student D:**
\[
P(D=\text{Pass}) = 4\beta
\]
---
### Step 2: Use independence
Because exam outcomes are independent, the likelihood function is the product of the individual probabilities:
\[
L(\beta)
= P(A=\text{Pass}) \cdot P(B=\text{Fail}) \cdot P(C=\text{Pass}) \cdot P(D=\text{Pass})
\]
---
### Step 3: Likelihood function
Substituting the probabilities:
\[
L(\beta) = (2\beta)(1 - 2\beta)(3\beta)(4\beta)
\]
or equivalently,
\[
\boxed{
L(\beta) = 24\,\beta^3(1 - 2\beta),
\qquad 0 < \beta < \tfrac{1}{4}
}
\]
# Problem 1(a) - Likelihood function L(p)
# Data from the table
n_art <- 5;  x_art <- 4   # 4 "Yes" out of 5 art students
n_bus <- 5;  x_bus <- 3   # 3 "Yes" out of 5 business students
# Model (from statement):
#P(Mac | Art) = p,  P(Mac | Business) = p/2
# Likelihood (ignoring constants):
# L(p) = p^(x_art) (1-p)^(n_art-x_art) * (p/2)^(x_bus) * (1 - p/2)^(n_bus-x_bus)
L_noConst <- function(p){
# valid parameter region: 0 < p < 1 (also ensures 1 - p/2 > 0)
if(any(p <= 0 | p >= 1)) return(0)
p^x_art * (1 - p)^(n_art - x_art) * (p/2)^x_bus * (1 - p/2)^(n_bus - x_bus)
}
# Define the log-likelihood function in R
l_p <- function(p) {
# Ensure p stays within the valid range (0, 1)
if(any(p <= 0 | p >= 1)) return(-Inf)
7*log(p) + log(1-p) + 2*log(1 - p/2) - 3*log(2)
}
# Define the derivative function
deriv_l <- function(p) { 7/p - 1/(1-p) - 2/(2-p) }
# Solve for p in the range (0, 1)
mle_p <- uniroot(deriv_l, interval = c(0.01, 0.99))$root
print(mle_p) # Should be approx 0.84688
# 1. Define p0 and retrieve your MLE
p0 <- 0.8
p_hat <- mle_p  # From your uniroot calculation (approx 0.84688)
# 2. Calculate the Likelihood Ratio R
# Using the log-likelihood function l_p already defined
log_R <- l_p(p0) - l_p(p_hat)
R <- exp(log_R)
# 3. Calculate the Deviance Statistic D (LRT statistic)
d_stat <- -2 * log(R)
# 4. Calculate the P-value [cite: 89, 90, 96]
# area.left.tail calculates area to the left
area.left.tail <- pchisq(d_stat, df = 1)
# pvalue is the area in the right tail
pvalue <- 1 - area.left.tail
# Output results
print(paste("Likelihood Ratio (R):", round(R, 4)))
print(paste("LRT Statistic (D):", round(d_stat, 4)))
print(paste("P-value:", round(pvalue, 4)))
# 1. Define inputs based on your problem
n <- 10           # Total sample size
# 2. Calculate Z-statistic
# se utilizes p0 under the null hypothesis as shown in teacher's slides
se <- sqrt((p0 * (1 - p0)) / n)
z <- (p_hat - p0) / se
# 3. Calculate P-value (Two-tailed)
# pnorm finds the area to the left; we multiply the tail area by 2
p_value <- 2 * (1 - pnorm(abs(z)))
# Print results to match the 'RR Console' style in slides
z
p_value
dl_db <- function(b) {
9 - (4*exp(2*b))/(1+exp(2*b)) - (3*exp(3*b))/(1+exp(3*b)) - (4*exp(4*b))/(1+exp(4*b))
}
mle_beta <- uniroot(dl_db, interval = c(0, 5))$root
mle_beta
dl_db <- function(b) {
9
- (4*exp(2*b))/(1+exp(2*b))
- (3*exp(3*b))/(1+exp(3*b))
- (4*exp(4*b))/(1+exp(4*b))
}
mle_beta <- uniroot(dl_db, interval = c(0, 5))$root
# Problem 1(a) - Likelihood function L(p)
# Data from the table
n_art <- 5;  x_art <- 4   # 4 "Yes" out of 5 art students
n_bus <- 5;  x_bus <- 3   # 3 "Yes" out of 5 business students
# Model (from statement):
#P(Mac | Art) = p,  P(Mac | Business) = p/2
# Likelihood (ignoring constants):
# L(p) = p^(x_art) (1-p)^(n_art-x_art) * (p/2)^(x_bus) * (1 - p/2)^(n_bus-x_bus)
L_noConst <- function(p){
# valid parameter region: 0 < p < 1 (also ensures 1 - p/2 > 0)
if(any(p <= 0 | p >= 1)) return(0)
p^x_art * (1 - p)^(n_art - x_art) * (p/2)^x_bus * (1 - p/2)^(n_bus - x_bus)
}
# Define the log-likelihood function in R
l_p <- function(p) {
# Ensure p stays within the valid range (0, 1)
if(any(p <= 0 | p >= 1)) return(-Inf)
7*log(p) + log(1-p) + 2*log(1 - p/2) - 3*log(2)
}
# Define the derivative function
deriv_l <- function(p) { 7/p - 1/(1-p) - 2/(2-p) }
# Solve for p in the range (0, 1)
mle_p <- uniroot(deriv_l, interval = c(0.01, 0.99))$root
print(mle_p) # Should be approx 0.84688
# 1. Define p0 and retrieve your MLE
p0 <- 0.8
p_hat <- mle_p  # From your uniroot calculation (approx 0.84688)
# 2. Calculate the Likelihood Ratio R
# Using the log-likelihood function l_p already defined
log_R <- l_p(p0) - l_p(p_hat)
R <- exp(log_R)
# 3. Calculate the Deviance Statistic D (LRT statistic)
d_stat <- -2 * log(R)
# 4. Calculate the P-value [cite: 89, 90, 96]
# area.left.tail calculates area to the left
area.left.tail <- pchisq(d_stat, df = 1)
# pvalue is the area in the right tail
pvalue <- 1 - area.left.tail
# Output results
print(paste("Likelihood Ratio (R):", round(R, 4)))
print(paste("LRT Statistic (D):", round(d_stat, 4)))
print(paste("P-value:", round(pvalue, 4)))
# 1. Define inputs based on your problem
n <- 10           # Total sample size
# 2. Calculate Z-statistic
# se utilizes p0 under the null hypothesis as shown in teacher's slides
se <- sqrt((p0 * (1 - p0)) / n)
z <- (p_hat - p0) / se
# 3. Calculate P-value (Two-tailed)
# pnorm finds the area to the left; we multiply the tail area by 2
p_value <- 2 * (1 - pnorm(abs(z)))
# Print results to match the 'RR Console' style in slides
z
p_value
dl_db <- function(b) {
9
- (4*exp(2*b))/(1+exp(2*b))
- (3*exp(3*b))/(1+exp(3*b))
- (4*exp(4*b))/(1+exp(4*b))
}
mle_beta <- uniroot(dl_db, interval = c(0, 5))$root
dl_db <- function(b) {
9
- (4*exp(2*b))/(1+exp(2*b))
- (3*exp(3*b))/(1+exp(3*b))
- (4*exp(4*b))/(1+exp(4*b))
}
mle_beta <- uniroot(dl_db, interval = c(0, 5))$root
dl_db <- function(b) {
9
- (4*exp(2*b))/(1+exp(2*b))
- (3*exp(3*b))/(1+exp(3*b))
- (4*exp(4*b))/(1+exp(4*b))
}
mle_beta <- uniroot(dl_db, interval = c(0, 5))$root
# Derivative of the log-likelihood
dl_db <- function(b) {
9
- (4*exp(2*b))/(1 + exp(2*b))
- (3*exp(3*b))/(1 + exp(3*b))
- (4*exp(4*b))/(1 + exp(4*b))
}
# Solve for beta in a reasonable interval
mle_beta <- uniroot(dl_db, interval = c(0, 5))$root
# Derivative of the log-likelihood
dl_db <- function(b) {
9
- (4*exp(2*b))/(1 + exp(2*b))
- (3*exp(3*b))/(1 + exp(3*b))
- (4*exp(4*b))/(1 + exp(4*b))
}
# Solve for beta in a reasonable interval
mle_beta <- uniroot(dl_db, interval = c(0, 5))$root
dl_db <- function(b) {
9
- (4*exp(2*b))/(1 + exp(2*b))
- (3*exp(3*b))/(1 + exp(3*b))
- (4*exp(4*b))/(1 + exp(4*b))
}
# Solve for beta in a reasonable interval
mle_beta <- uniroot(dl_db, interval = c(0, 5))$root
# Definimos la derivada de la función de log-verosimilitud
dl_db <- function(b) {
9 - (4 * exp(2 * b)) / (1 + exp(2 * b)) -
(3 * exp(3 * b)) / (1 + exp(3 * b)) -
(4 * exp(4 * b)) / (1 + exp(4 * b))
}
# uniroot busca el valor de b que hace que la derivada sea 0
mle_beta_result <- uniroot(dl_db, interval = c(0, 5))
mle_beta <- mle_beta_result$root
# Mostramos el resultado
mle_beta
# Definimos la derivada de la función de log-verosimilitud
dl_db <- function(b) {
9 - (4 * exp(2 * b)) / (1 + exp(2 * b))
- (3 * exp(3 * b)) / (1 + exp(3 * b))
-(4 * exp(4 * b)) / (1 + exp(4 * b))
}
# uniroot busca el valor de b que hace que la derivada sea 0
mle_beta_result <- uniroot(dl_db, interval = c(0, 5))
# Definimos la derivada de la función de log-verosimilitud
dl_db <- function(b) {
9 - (4 * exp(2 * b)) / (1 + exp(2 * b)) -
(3 * exp(3 * b)) / (1 + exp(3 * b)) -
(4 * exp(4 * b)) / (1 + exp(4 * b))
}
# uniroot busca el valor de b que hace que la derivada sea 0
mle_beta_result <- uniroot(dl_db, interval = c(0, 5))
mle_beta <- mle_beta_result$root
# Mostramos el resultado
mle_beta
# Definimos la derivada de la función de log-verosimilitud
dl_db <- function(b) {
9 - ((4 * exp(2 * b)) / (1 + exp(2 * b))) -
((3 * exp(3 * b)) / (1 + exp(3 * b))) -
((4 * exp(4 * b)) / (1 + exp(4 * b)))
}
# uniroot busca el valor de b que hace que la derivada sea 0
mle_beta_result <- uniroot(dl_db, interval = c(0, 5))
mle_beta <- mle_beta_result$root
# Mostramos el resultado
mle_beta
# 1. Define the log-likelihood function from part (b)
l_beta <- function(b) {
9*b - 2*log(1 + exp(2*b)) - log(1 + exp(3*b)) - log(1 + exp(4*b))
}
# 2. Assign values
beta_hat <- 0.5222167
beta_0 <- 0.2
# 3. Calculate D and P-value
D_stat <- 2 * (l_beta(beta_hat) - l_beta(beta_0))
p_val_lrt <- 1 - pchisq(D_stat, df = 1)
cat("LRT Statistic (D):", D_stat, "\n")
cat("P-value:", p_val_lrt)
# Problem 1(a) - Likelihood function L(p)
# Data from the table
n_art <- 5;  x_art <- 4   # 4 "Yes" out of 5 art students
n_bus <- 5;  x_bus <- 3   # 3 "Yes" out of 5 business students
# Model (from statement):
#P(Mac | Art) = p,  P(Mac | Business) = p/2
# Likelihood (ignoring constants):
# L(p) = p^(x_art) (1-p)^(n_art-x_art) * (p/2)^(x_bus) * (1 - p/2)^(n_bus-x_bus)
L_noConst <- function(p){
# valid parameter region: 0 < p < 1 (also ensures 1 - p/2 > 0)
if(any(p <= 0 | p >= 1)) return(0)
p^x_art * (1 - p)^(n_art - x_art) * (p/2)^x_bus * (1 - p/2)^(n_bus - x_bus)
}
# Define the log-likelihood function in R
l_p <- function(p) {
# Ensure p stays within the valid range (0, 1)
if(any(p <= 0 | p >= 1)) return(-Inf)
7*log(p) + log(1-p) + 2*log(1 - p/2) - 3*log(2)
}
# Define the derivative function
deriv_l <- function(p) { 7/p - 1/(1-p) - 2/(2-p) }
# Solve for p in the range (0, 1)
mle_p <- uniroot(deriv_l, interval = c(0.01, 0.99))$root
print(mle_p) # Should be approx 0.84688
# 1. Define p0 and retrieve your MLE
p0 <- 0.8
p_hat <- mle_p  # From your uniroot calculation (approx 0.84688)
# 2. Calculate the Likelihood Ratio R
# Using the log-likelihood function l_p already defined
log_R <- l_p(p0) - l_p(p_hat)
R <- exp(log_R)
# 3. Calculate the Deviance Statistic D (LRT statistic)
d_stat <- -2 * log(R)
# 4. Calculate the P-value [cite: 89, 90, 96]
# area.left.tail calculates area to the left
area.left.tail <- pchisq(d_stat, df = 1)
# pvalue is the area in the right tail
pvalue <- 1 - area.left.tail
# Output results
print(paste("Likelihood Ratio (R):", round(R, 4)))
print(paste("LRT Statistic (D):", round(d_stat, 4)))
print(paste("P-value:", round(pvalue, 4)))
# 1. Define inputs based on your problem
n <- 10           # Total sample size
# 2. Calculate Z-statistic
# se utilizes p0 under the null hypothesis as shown in teacher's slides
se <- sqrt((p0 * (1 - p0)) / n)
z <- (p_hat - p0) / se
# 3. Calculate P-value (Two-tailed)
# pnorm finds the area to the left; we multiply the tail area by 2
p_value <- 2 * (1 - pnorm(abs(z)))
# Print results to match the 'RR Console' style in slides
z
p_value
# Definimos la derivada de la función de log-verosimilitud
dl_db <- function(b) {
9 - ((4 * exp(2 * b)) / (1 + exp(2 * b))) -
((3 * exp(3 * b)) / (1 + exp(3 * b))) -
((4 * exp(4 * b)) / (1 + exp(4 * b)))
}
# uniroot busca el valor de b que hace que la derivada sea 0
mle_beta_result <- uniroot(dl_db, interval = c(0, 5))
mle_beta <- mle_beta_result$root
# Mostramos el resultado
mle_beta
# 1. Define the log-likelihood function from part (b)
l_beta <- function(b) {
9*b - 2*log(1 + exp(2*b)) - log(1 + exp(3*b)) - log(1 + exp(4*b))
}
# 2. Assign values
beta_hat <- 0.5222167
beta_0 <- 0.2
# 3. Calculate D and P-value
D_stat <- 2 * (l_beta(beta_hat) - l_beta(beta_0))
p_val_lrt <- 1 - pchisq(D_stat, df = 1)
cat("LRT Statistic (D):", D_stat, "\n")
cat("P-value:", p_val_lrt)
# Define the negative log-likelihood function
neg_log_lik <- function(beta) {
-(9*beta - (2*log(1 + exp(2*beta)) + log(1 + exp(3*beta)) + log(1 + exp(4*beta))))
}
# Numerical optimization
mle_result <- optim(par = 0.5, fn = neg_log_lik, method = "BFGS")
hat_beta <- mle_result$par
cat("The Maximum Likelihood Estimate (MLE) for beta is:", round(hat_beta, 4))
knitr::opts_chunk$set(echo = TRUE)
mydata = read.csv( './loan_default.csv' )
names( mydata )
# Define the predictor variable (x), which is "Hours" in this case
x = mydata$Income
# Define the response variable (y) - the variable we are predicting its outcomes,
# which is the "Grade"
y = mydata$Loan.default.indicator
# The 'y' variable currently contains labels (e.g., "F", "P")
# We need to convert this into a categorical variable with two levels: Pass and Fail
# To do that, we can use the factor(...) function
y = factor(mydata$Loan.default.indicator)
# In R, the second level of a factor is considered the "success" category by default.
# In this case, "Pass" is considered the success.
# To confirm the factor levels, use the levels() function
levels(y)
fitted.model = glm(y ~ x, family = binomial)
#Use the summary function to get the regression ouput
summary(fitted.model)
names( mydata )
# output:[1] "Income"                 "Loan.default.indicator"
str(mydata)
# The 'y' variable currently contains labels (e.g., "F", "P")
# To do that, we can use the factor(...) function
y = factor(mydata$Loan.default.indicator)
# In R, the second level of a factor is considered the "success" category by default.
# In this case, "Pass" is considered the success.
# To confirm the factor levels, use the levels() function
levels(y)
fitted.model = glm(y ~ x, family = binomial)
#Use the summary function to get the regression ouput
summary(fitted.model)
# against the null model (intercept only)
anova(
fitted.model.no.predictor, # the null model (without predictor)
fitted.model,
# the alternative model (with predictor)
test = "Chisq"
)
knitr::opts_chunk$set(echo = TRUE)
mydata = read.csv( './loan_default.csv' )
names( mydata )
# output:[1] "Income"                 "Loan.default.indicator"
str(mydata)
# 'data.frame':	240 obs. of  2 variables:
#  $ Income                : num  20.1 20.1 20.2 20.6 21.2 ...
#  $ Loan.default.indicator: int  0 1 1 1 1 0 0 1 1 1 ...
# Define the predictor variable (x),
x = mydata$Income
# Define the response variable (y)
y = mydata$Loan.default.indicator
# The 'y' variable currently contains labels (e.g., "F", "P")
# To do that, we can use the factor(...) function
y = factor(mydata$Loan.default.indicator)
# In R, the second level of a factor is considered the "success" category by default.
# In this case, "Pass" is considered the success.
# To confirm the factor levels, use the levels() function
levels(y)
fitted.model = glm(y ~ x, family = binomial)
#Use the summary function to get the regression ouput
summary(fitted.model)
fitted.model = glm(y ~ x, family = binomial)
# The model includes only an intercept (denoted by "1").
fitted.model.no.predictor = glm( y ~ 1, family = binomial)
#Use the summary function to get the regression ouput
summary(fitted.model)
# against the null model (intercept only)
anova(
fitted.model.no.predictor, # the null model (without predictor)
fitted.model,
# the alternative model (with predictor)
test = "Chisq"
)
beta0 <- 0.823610
beta1 <- -0.043751
odds_20 <- exp(beta0 + beta1 * 20)
odds_20
mydata = read.csv( './loan_default.csv' )
names( mydata )
# output:[1] "Income"                 "Loan.default.indicator"
str(mydata)
# 'data.frame':	240 obs. of  2 variables:
#  $ Income                : num  20.1 20.1 20.2 20.6 21.2 ...
#  $ Loan.default.indicator: int  0 1 1 1 1 0 0 1 1 1 ...
# Define the predictor variable (x),
x = mydata$Income
# Define the response variable (y)
y = mydata$Loan.default.indicator
# The 'y' variable currently contains labels (e.g., "F", "P")
# To do that, we can use the factor(...) function
y = factor(mydata$Loan.default.indicator)
# In R, the second level of a factor is considered the "success" category by default.
# In this case, "Pass" is considered the success.
# To confirm the factor levels, use the levels() function
levels(y)
fitted.model = glm(y ~ x, family = binomial)
#Use the summary function to get the regression ouput
summary(fitted.model)
fitted.model = glm(y ~ x, family = binomial)
# The model includes only an intercept (denoted by "1").
fitted.model.no.predictor = glm( y ~ 1, family = binomial)
#Use the summary function to get the regression ouput
summary(fitted.model.no.predictor)
mydata = read.csv( './loan_default.csv' )
names( mydata )
# output:[1] "Income"                 "Loan.default.indicator"
str(mydata)
# 'data.frame':	240 obs. of  2 variables:
#  $ Income                : num  20.1 20.1 20.2 20.6 21.2 ...
#  $ Loan.default.indicator: int  0 1 1 1 1 0 0 1 1 1 ...
# Define the predictor variable (x),
x = mydata$Income
# Define the response variable (y)
y = mydata$Loan.default.indicator
# The 'y' variable currently contains labels (e.g., "F", "P")
# To do that, we can use the factor(...) function
y = factor(mydata$Loan.default.indicator)
# In R, the second level of a factor is considered the "success" category by default.
# In this case, "Pass" is considered the success.
# To confirm the factor levels, use the levels() function
levels(y)
fitted.model = glm(y ~ x, family = binomial)
#Use the summary function to get the regression ouput
summary(fitted.model)
coeficients=coef(fitted.model)
print(fitted.model)
coeficients=cofin(fitted.model)
coeficients=Coffin(fitted.model)
print(fitted.model)
knitr::opts_chunk$set(echo = TRUE)
mydata = read.csv( './loan_default.csv' )
names( mydata )
# output:[1] "Income"                 "Loan.default.indicator"
str(mydata)
# 'data.frame':	240 obs. of  2 variables:
#  $ Income                : num  20.1 20.1 20.2 20.6 21.2 ...
#  $ Loan.default.indicator: int  0 1 1 1 1 0 0 1 1 1 ...
# Define the predictor variable (x),
x = mydata$Income
# Define the response variable (y)
y = mydata$Loan.default.indicator
# The 'y' variable currently contains labels (e.g., "F", "P")
# To do that, we can use the factor(...) function
y = factor(mydata$Loan.default.indicator)
# In R, the second level of a factor is considered the "success" category by default.
# In this case, "Pass" is considered the success.
# To confirm the factor levels, use the levels() function
levels(y)
fitted.model = glm(y ~ x, family = binomial)
#Use the summary function to get the regression ouput
summary(fitted.model)
coeficients=Coffin(fitted.model)
print(fitted.model)
fitted.model = glm(y ~ x, family = binomial)
# The model includes only an intercept (denoted by "1").
fitted.model.no.predictor = glm( y ~ 1, family = binomial)
#Use the summary function to get the regression ouput
summary(fitted.model.no.predictor)
# against the null model (intercept only)
anova(
fitted.model.no.predictor, # the null model (without predictor)
fitted.model,
# the alternative model (with predictor)
test = "Chisq"
)
beta0 <- 0.823610
beta1 <- -0.043751
odds_20 <- exp(beta0 + beta1 * 20)
odds_20
coeficients=coefficients(fitted.model)
print(fitted.model)
